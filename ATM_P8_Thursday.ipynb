{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ATM_P8_Thursday.ipynb",
      "provenance": [],
      "mount_file_id": "13dx2Wj6Ez5xAuUbdIvX-eAMYtNZyVHES",
      "authorship_tag": "ABX9TyMBCvoSNi8OYOh8tkJ8wg+d",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/laurencefrank/Applied-Text-Mining/blob/main/ATM_P8_Thursday.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aht250pZIIe9"
      },
      "source": [
        "## Applied Text Mining, Utrecht Summerschool 26 - 29 July 2021\n",
        "\n",
        "Thursday 29 July\n",
        "\n",
        "Practical 8: Neural Machine Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-MPeGPlJYIx"
      },
      "source": [
        "In this practical, we will create models for neural machine translation. Today we are curious to see how a simple deep learning based model translates a sentence into its counterpart. See these examples:\n",
        "\n",
        "\n",
        "The objective from this practical is to convert a Dutch sentence to its English counterpart using a Neural Machine Translation (NMT) system. We will implement this task by building a simple Sequence-to-Sequence model with the help of Keras library.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu9z-IkFH4PX"
      },
      "source": [
        "import string\n",
        "import re\n",
        "import statistics\n",
        "from numpy import array, argmax, random, take\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, Bidirectional, RepeatVector, TimeDistributed\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "# matplotlib inline\n",
        "pd.set_option('display.max_colwidth', 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtiXY6d1Lb4H"
      },
      "source": [
        "## Let's get started!\n",
        "\n",
        "1. In this practical we will use a data set of tab-delimited Bilingual Sentence Pairs from http://www.manythings.org/anki/. Use the following two functions (read_text and to_lines) and read the nld.txt data set (also provided in the course webpage next to the practial link). This data set contains phrases in Dutch with their translation in English. Convert the text sequences to an array and check the first items in your array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26obCvkCLbLB"
      },
      "source": [
        "# function to read raw text file\n",
        "def read_text(filename):\n",
        "    # open the file\n",
        "    file = open(filename, mode='rt', encoding='utf-8')\n",
        "    # read all text\n",
        "    text = file.read()\n",
        "    file.close()\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bM2Yb_kSLiSd"
      },
      "source": [
        "# split a text into sentences\n",
        "def to_lines(text):\n",
        "    sents = text.strip().split('\\n')\n",
        "    sents = [i.split('\\t') for i in sents]\n",
        "    return sents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRlIJ-XMLkuo"
      },
      "source": [
        "data = read_text(\"/content/drive/MyDrive/Colab Notebooks/AppliedTextMiningSummerschoolUtrechtJuly2021/nld-eng/nld.txt\")\n",
        "nld_eng = to_lines(data)\n",
        "nld_eng = array(nld_eng)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5i0ddR6Lmnd",
        "outputId": "c32b6009-cdfa-4ee7-9bcf-b0654e16c660"
      },
      "source": [
        "nld_eng.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(54972, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xaGi1tS6Lrem",
        "outputId": "bedd464e-2d13-4508-e103-5bdb182c22be"
      },
      "source": [
        "nld_eng"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['Go.', 'Lopen!',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #7764436 (LinguisticFusion)'],\n",
              "       ['Go.', 'Vooruit.',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #7915821 (Elsofie)'],\n",
              "       ['Hi.', 'Hoi.',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #537889 (Dorenda)'],\n",
              "       ...,\n",
              "       ['Always use distilled water in steam irons because using ordinary water will cause a mineral build-up over time that will clog the steam holes.',\n",
              "        'Gebruik altijd gedistilleerd water in stoomstrijkijzers, want gewoon water zorgt voor mineraalophoping dat de stoomgaatjes na verloop van tijd verstopt.',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #3020388 (Delian) & #3037091 (Citrine)'],\n",
              "       [\"If you translate from your second language into your own native language, rather than the other way around, you're less likely to make mistakes.\",\n",
              "        'Als je vanuit je tweede taal naar je eigen moedertaal vertaalt, in plaats van andersom, maak je minder snel fouten.',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #1230823 (CK) & #8627687 (MarijnKp)'],\n",
              "       [\"If someone who doesn't know your background says that you sound like a native speaker, it means they probably noticed something about your speaking that made them realize you weren't a native speaker. In other words, you don't really sound like a native speaker.\",\n",
              "        'Als iemand die je achtergrond niet kent zegt dat je klinkt als een moedertaalspreker betekent dat dat diegene waarschijnlijk iets in je spreken opgemerkt heeft dat hem deed realiseren dat je geen moedertaalspreker bent. Met andere woorden, je klinkt niet echt als een moedertaalspreker.',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #953936 (CK) & #1056762 (ReneeMona)']],\n",
              "      dtype='<U286')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_cQs0xqMeP_"
      },
      "source": [
        "## Pre-processing\n",
        "\n",
        "2. Use the `maketrans()` function to remove punctuations from the nld_enp object. The `maketrans()` function is a function from the library `str` that is used to construct a transition table, i.e. that it specifies a list of characters that need to be replaced in a string or the characters that need to be deleted from the string. To use this transition table, you can use the `translate()` function and apply that on a string. It is also possible to use these functions to remove the punctuations. Similar to the example below, apply the `maketrans()` function to remove punctuations from the `nld_eng` array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AozE4kkuOcrF",
        "outputId": "14bc49cc-1109-47ac-ee40-e66cec32e5d3"
      },
      "source": [
        "### Here you see an example on how to use the maketrans() function ###\n",
        "# specify the list of characters that need to be replaced\n",
        "str1 = \"mtex\"\n",
        "\n",
        "# specify the list of characters with which the characters need to be replaced\n",
        "str2 = \"dwoo\"\n",
        "\n",
        "# specify the list of characters that needs to be deleted\n",
        "str3 = \"u\"\n",
        "\n",
        "# target string \n",
        "temp_str = \"text mining\"\n",
        "\n",
        "# using maketrans() to construct a translate table\n",
        "table = temp_str.maketrans(str1, str2, str3)\n",
        "  \n",
        "# Printing original string \n",
        "print (\"The string before translating is : \", end =\"\")\n",
        "print (temp_str)\n",
        "  \n",
        "# using translate() to make translations.\n",
        "print (\"The string after translating is : \", end =\"\")\n",
        "print (temp_str.translate(table))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The string before translating is : text mining\n",
            "The string after translating is : woow dining\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i10v2mPDOhcb"
      },
      "source": [
        "Note that it is possible to replace one character with two or more. You need to supply a dict as argument to maketrans()."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQFFbmOwOgDZ"
      },
      "source": [
        "nld_eng[:,0] = [s.translate(str.maketrans('', '', string.punctuation)) for s in nld_eng[:,0]]\n",
        "nld_eng[:,1] = [s.translate(str.maketrans('', '', string.punctuation)) for s in nld_eng[:,1]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6jHEzseTr7C",
        "outputId": "7fa94f6d-0fc0-4553-f768-a692bc853578"
      },
      "source": [
        "nld_eng"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['Go', 'Lopen',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #7764436 (LinguisticFusion)'],\n",
              "       ['Go', 'Vooruit',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #7915821 (Elsofie)'],\n",
              "       ['Hi', 'Hoi',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #537889 (Dorenda)'],\n",
              "       ...,\n",
              "       ['Always use distilled water in steam irons because using ordinary water will cause a mineral buildup over time that will clog the steam holes',\n",
              "        'Gebruik altijd gedistilleerd water in stoomstrijkijzers want gewoon water zorgt voor mineraalophoping dat de stoomgaatjes na verloop van tijd verstopt',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #3020388 (Delian) & #3037091 (Citrine)'],\n",
              "       ['If you translate from your second language into your own native language rather than the other way around youre less likely to make mistakes',\n",
              "        'Als je vanuit je tweede taal naar je eigen moedertaal vertaalt in plaats van andersom maak je minder snel fouten',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #1230823 (CK) & #8627687 (MarijnKp)'],\n",
              "       ['If someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker In other words you dont really sound like a native speaker',\n",
              "        'Als iemand die je achtergrond niet kent zegt dat je klinkt als een moedertaalspreker betekent dat dat diegene waarschijnlijk iets in je spreken opgemerkt heeft dat hem deed realiseren dat je geen moedertaalspreker bent Met andere woorden je klinkt niet echt als een moedertaalspreker',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #953936 (CK) & #1056762 (ReneeMona)']],\n",
              "      dtype='<U286')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foKqcjSeWAlF"
      },
      "source": [
        "3. Convert all words into their lowercase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClfL1rNIWBaw"
      },
      "source": [
        "# convert to lowercase\n",
        "for i in range(len(nld_eng)):\n",
        "    nld_eng[i,0] = nld_eng[i,0].lower()    \n",
        "    nld_eng[i,1] = nld_eng[i,1].lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4NBIvVRWsMF",
        "outputId": "e85d90d5-cb18-4e22-88b6-6ff63eb3a4dd"
      },
      "source": [
        "nld_eng"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([['go', 'lopen',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #7764436 (LinguisticFusion)'],\n",
              "       ['go', 'vooruit',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #7915821 (Elsofie)'],\n",
              "       ['hi', 'hoi',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #537889 (Dorenda)'],\n",
              "       ...,\n",
              "       ['always use distilled water in steam irons because using ordinary water will cause a mineral buildup over time that will clog the steam holes',\n",
              "        'gebruik altijd gedistilleerd water in stoomstrijkijzers want gewoon water zorgt voor mineraalophoping dat de stoomgaatjes na verloop van tijd verstopt',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #3020388 (Delian) & #3037091 (Citrine)'],\n",
              "       ['if you translate from your second language into your own native language rather than the other way around youre less likely to make mistakes',\n",
              "        'als je vanuit je tweede taal naar je eigen moedertaal vertaalt in plaats van andersom maak je minder snel fouten',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #1230823 (CK) & #8627687 (MarijnKp)'],\n",
              "       ['if someone who doesnt know your background says that you sound like a native speaker it means they probably noticed something about your speaking that made them realize you werent a native speaker in other words you dont really sound like a native speaker',\n",
              "        'als iemand die je achtergrond niet kent zegt dat je klinkt als een moedertaalspreker betekent dat dat diegene waarschijnlijk iets in je spreken opgemerkt heeft dat hem deed realiseren dat je geen moedertaalspreker bent met andere woorden je klinkt niet echt als een moedertaalspreker',\n",
              "        'CC-BY 2.0 (France) Attribution: tatoeba.org #953936 (CK) & #1056762 (ReneeMona)']],\n",
              "      dtype='<U286')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeKrdG6lW11u"
      },
      "source": [
        "## Text to Sequence\n",
        "\n",
        "4. What is the maximum length of a sentence in each of the Dutch and English sets? What about the average length?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNeJclzaW8_n"
      },
      "source": [
        "# empty lists\n",
        "eng_l = []\n",
        "nld_l = []\n",
        "# populate the lists with sentence lengths\n",
        "for i in nld_eng[:,0]:\n",
        "    eng_l.append(len(i.split()))\n",
        "\n",
        "for i in nld_eng[:,1]:\n",
        "    nld_l.append(len(i.split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "51-xPR7iXC4e",
        "outputId": "98a942f8-d9f8-4884-937c-1428feff9b90"
      },
      "source": [
        "length_df = pd.DataFrame({'eng':eng_l, 'nld':nld_l})\n",
        "length_df.hist(bins = 30)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkgHF289XLMk",
        "outputId": "4571d50d-8589-4c08-f40c-87a7e7cbe95a"
      },
      "source": [
        "max(eng_l)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qCirI1FXOAt",
        "outputId": "becbeb4c-47be-49b5-f0a7-94c4fec481d8"
      },
      "source": [
        "max(nld_l)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "44"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZrLDcauXp3g",
        "outputId": "00187d9c-7af8-4b69-f4b4-3b295c2c9fcb"
      },
      "source": [
        "type(nld_l)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ry975gIX0o9",
        "outputId": "bc30b0b1-8773-4bb3-b457-61f6357f704a"
      },
      "source": [
        "statistics.mean(nld_l)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.853307138179437"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HbSagDVdX4M4",
        "outputId": "8d62d44b-2d25-4854-cc0a-dfa242f6c1f9"
      },
      "source": [
        "statistics.mean(eng_l)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.619569962890199"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1kNUkmoX9aW"
      },
      "source": [
        "Coincidentally the maximum length of the Dutch sentences and that of the English phrases is equal to 44.\n",
        "\n",
        "5. Use the train_test_split function from sklearn to split the data set into training (80%) and test (20%) sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnwirmNwX_KP"
      },
      "source": [
        "train, test = train_test_split(nld_eng, test_size=0.2, random_state=321)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97naioxfYFxx"
      },
      "source": [
        "6. Time to tokenize the sentences. Use the Tokenizer function from Keras and fit the sentences. Find out about the vocabulary size for the Dutch and English sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdbtWnQWYHSK",
        "outputId": "ba62878f-7ad8-4338-9bf2-e1e9b77e1b56"
      },
      "source": [
        "# prepare english tokenizer\n",
        "eng_tokenizer = Tokenizer()\n",
        "eng_tokenizer.fit_on_texts(nld_eng[:, 0])\n",
        "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
        "print('English Vocabulary Size: %d' % eng_vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocabulary Size: 9073\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNhRWKs3ZTo8",
        "outputId": "a61a7d8d-6d54-4736-c4b7-635799b5ffd3"
      },
      "source": [
        "# prepare Dutch tokenizer\n",
        "nld_tokenizer = Tokenizer()\n",
        "nld_tokenizer.fit_on_texts(nld_eng[:, 1])\n",
        "nld_vocab_size = len(nld_tokenizer.word_index) + 1\n",
        "print('Dutch Vocabulary Size: %d' % nld_vocab_size)\n",
        "# print('Dutch Vocabulary Size:', nld_vocab_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dutch Vocabulary Size: 12794\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpVVX2p3ZfLd"
      },
      "source": [
        "7. Write a function to convert tokens into sequences using an argument for maximum sentence length. Other input arguments to this function are tokenizer and sentences, and its output will be sequences of tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfC-iwUiZilJ"
      },
      "source": [
        "# encode and pad sequences\n",
        "def encode_sequences(tokenizer, maximum_length, sentences):\n",
        "    # integer encode sequences\n",
        "    seq = tokenizer.texts_to_sequences(sentences)\n",
        "    # pad sequences with 0 values\n",
        "    seq = pad_sequences(seq, maxlen=maximum_length, padding='post')\n",
        "    return seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ibhk_I9iuM2"
      },
      "source": [
        "8. Convert your tokenized training data into sequences. Use a maximum length of 20 and name the dataframes train_X and train_Y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1I2qj2JViveA"
      },
      "source": [
        "eng_length = 40\n",
        "nld_length = 40\n",
        "# prepare training data\n",
        "train_X = encode_sequences(nld_tokenizer, nld_length, train[:, 1])\n",
        "train_Y = encode_sequences(eng_tokenizer, eng_length, train[:, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EI-uEQAi0Yv"
      },
      "source": [
        "9. In the same way, convert your tokenized test data into sequences and name the dataframs test_X and test_Y."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_NBTBl7i2fT"
      },
      "source": [
        "# prepare test data\n",
        "test_X = encode_sequences(nld_tokenizer, nld_length, test[:, 1])\n",
        "test_Y = encode_sequences(eng_tokenizer, eng_length, test[:, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWqgCTnwi6VO"
      },
      "source": [
        "# Neural Network Model\n",
        "\n",
        "10. Defince a `Seq2Seq` model architecture using an Embedding layer as the input layer, an LSTM layer as our encoder and another LSTM layer followed by a Dense layer as the decoder. Make this a function and name it `build_model()`. Define different input arguments for your function including the embedding_size and the number of LSTM units."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoIVG-ZrjGZ0"
      },
      "source": [
        "# build NMT model\n",
        "def build_model(in_vocab, out_vocab, in_timesteps, out_timesteps, embedding_size, LSTMunits):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(in_vocab, embedding_size, input_length=in_timesteps, mask_zero=True))\n",
        "    model.add(LSTM(LSTMunits))\n",
        "    model.add(RepeatVector(out_timesteps))\n",
        "    model.add(LSTM(LSTMunits, return_sequences=True))\n",
        "    model.add(Dense(out_vocab, activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4mwC-c0jThS"
      },
      "source": [
        "11. Create a model by calling the function with embedding_size of 300 and 512 units for the LSTM layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Stuq_GHCjWqj"
      },
      "source": [
        "model = build_model(nld_vocab_size, \n",
        "                    eng_vocab_size, \n",
        "                    nld_length, \n",
        "                    eng_length, \n",
        "                    300, \n",
        "                    512)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYD_GQBTjcRP"
      },
      "source": [
        "12. Compile the model with the RMSprop optimizer and sparse_categorical_crossentropy for loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dwuLPR0jemX",
        "outputId": "03231d6a-8c4b-4ede-a787-0019ccb0ae37"
      },
      "source": [
        "model.compile(optimizer=optimizers.RMSprop(learning_rate=0.001), \n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 40, 300)           3838200   \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 512)               1665024   \n",
            "_________________________________________________________________\n",
            "repeat_vector (RepeatVector) (None, 40, 512)           0         \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (None, 40, 512)           2099200   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 40, 9073)          4654449   \n",
            "=================================================================\n",
            "Total params: 12,256,873\n",
            "Trainable params: 12,256,873\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOUj3sbmjxEv"
      },
      "source": [
        "Note that we have used 'sparse_categorical_crossentropy' as the loss function as it allows us to use the target sequence as it is instead of one-hot encoded format. One-hot encoding the target sequences with such a huge vocabulary might consume your system's entire memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tkAwLgNvjznq"
      },
      "source": [
        "13. Fit the model with your desired number of epochs (e.g. 1 :), `validation_split` of 0.2, and `batch_size` of 128. You can use smaller values for the number of LSTM units (100) and embedding size (50) if it takes a lot of time to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "8110FFe5jyho",
        "outputId": "1a51bfd5-11fd-4909-8d89-a52bb67176ae"
      },
      "source": [
        "history = model.fit(train_X, train_Y.reshape(train_Y.shape[0], train_Y.shape[1], 1), \n",
        "                    epochs=2, \n",
        "                    batch_size=128, \n",
        "                    validation_split = 0.2,\n",
        "                    verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "275/275 [==============================] - 1877s 7s/step - loss: 1.4973 - accuracy: 0.8454 - val_loss: 0.8770 - val_accuracy: 0.8699\n",
            "Epoch 2/2\n",
            "275/275 [==============================] - 1837s 7s/step - loss: 0.8525 - accuracy: 0.8727 - val_loss: 0.8161 - val_accuracy: 0.8791\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kZxfckglRp9"
      },
      "source": [
        "With ModelCheckpoint you can save your best model during the training. We used this option of Keras to run our neural translator for 15 epochs and save the model for your use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "231UXZoVlTRM"
      },
      "source": [
        "#filename = 'model.15Epochs'\n",
        "#checkpoint = ModelCheckpoint(filename, \n",
        "#                             monitor='val_loss', \n",
        "#                             verbose=1, \n",
        "#                             save_best_only=True, \n",
        "#                             mode='min')\n",
        "\n",
        "# history = model.fit(train_X, train_Y.reshape(train_Y.shape[0], train_Y.shape[1], 1),\n",
        "#                     epochs=15,\n",
        "#                     batch_size=128, \n",
        "#                     validation_split = 0.2,\n",
        "#                     callbacks=[checkpoint],\n",
        "#                     verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfnYZrQJtNWX"
      },
      "source": [
        "14. Plot the accuracy and loss of your model for the training and validations sets.\n",
        "\n",
        "We will use the plot_history function from the previous practical:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rthkQYdjtPsb"
      },
      "source": [
        "plt.style.use('ggplot')\n",
        "def plot_history(history, val=0):\n",
        "    acc = history.history['accuracy']\n",
        "    if val == 1:\n",
        "        val_acc = history.history['val_accuracy'] # we can add a validation set in our fit function with nn\n",
        "    loss = history.history['loss']\n",
        "    if val == 1:\n",
        "        val_loss = history.history['val_loss']\n",
        "    x = range(1, len(acc) + 1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(x, acc, 'b', label='Training accuracy')\n",
        "    if val == 1:\n",
        "        plt.plot(x, val_acc, 'r', label='Validation accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.title('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(x, loss, 'b', label='Training loss')\n",
        "    if val == 1:\n",
        "        plt.plot(x, val_loss, 'r', label='Validation loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.title('Loss')\n",
        "    plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8Zle79AtT_D"
      },
      "source": [
        "plot_history(history, val=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Una5QRpptWTY"
      },
      "source": [
        "You can also use the following code without the plot_history function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnUAe8S7tX-8"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['train','validation'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JUYGOHJtcWX"
      },
      "source": [
        "15. Predict translations for the test set.\n",
        "\n",
        "You can continue using your model or you can use the following code to load the trained model with 10 epochs. Remember that our chosen architecture for a deep learning encoder-decoder translation model is a very simple architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vkgJn_StejQ"
      },
      "source": [
        "# model = load_model('model.15Epochs')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6U9IyMdtgy1"
      },
      "source": [
        "preds = model.predict_classes(test_X.reshape((test_X.shape[0],test_X.shape[1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JO0usDbPtnz-"
      },
      "source": [
        "preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMOb1AkItqph"
      },
      "source": [
        "As you noticed preds are only indices of words, so we need to convert them to words to be able to read them.\n",
        "\n",
        "16. Use the sequences_to_texts function to convert an index to a word on your predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JlGQ0iqbtsRx"
      },
      "source": [
        "preds_text = eng_tokenizer.sequences_to_texts(preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n_lLYu-t1S0"
      },
      "source": [
        "preds_text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b9z_Ll0uDko"
      },
      "source": [
        "17. Create a new dataframe with three columns where you show the input Dutch text of the test set, the actual output, and your predictions. Use the sample() function with your dataframe to randomly check some of the lines."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4Hv_HPquFtX"
      },
      "source": [
        "pred_df = pd.DataFrame({'input' : test[:, 1], 'actual output' : test[:,0], 'predicted output' : preds_text})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-SlFx9lcuHGU"
      },
      "source": [
        "#pd.set_option('display.max_colwidth', 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7S_I2ylKuI85"
      },
      "source": [
        "pred_df.head(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiA4BOvOuKkw"
      },
      "source": [
        "pred_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hrj--FxluOeT"
      },
      "source": [
        "pred_df.sample(15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bb3Nh047uRFH"
      },
      "source": [
        "18. Tatoeba.org (https://tatoeba.org/en/downloads) has a large database of example sentences translated into many languages by volunteers. To have a better data for your neural machine translator you can use this tool to generate and download customized sentence pairs. For example it has more than one million sentence pairs translated from Dutch to English. This time, try to tune the hyperparameters and add an attention layer after the dense layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9tFMUwWuT1w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}